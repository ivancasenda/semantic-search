# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: pipeline/components/vertex_ai_bulk_inferrer/public/proto/model_spec.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\nIpipeline/components/vertex_ai_bulk_inferrer/public/proto/model_spec.proto\x12\x13pipeline.model_spec\"\xda\x02\n\x11InferenceSpecType\x12?\n\x10saved_model_spec\x18\x01 \x01(\x0b\x32#.pipeline.model_spec.SavedModelSpecH\x00\x12_\n!ai_platform_prediction_model_spec\x18\x02 \x01(\x0b\x32\x32.pipeline.model_spec.AIPlatformPredictionModelSpecH\x00\x12[\n\x1fvertex_ai_prediction_model_spec\x18\x03 \x01(\x0b\x32\x30.pipeline.model_spec.VertexAIPredictionModelSpecH\x00\x12>\n\x10\x62\x61tch_parameters\x18\x04 \x01(\x0b\x32$.pipeline.model_spec.BatchParametersB\x06\n\x04type\"I\n\x0eSavedModelSpec\x12\x12\n\nmodel_path\x18\x01 \x01(\t\x12\x16\n\x0esignature_name\x18\x02 \x03(\t\x12\x0b\n\x03tag\x18\x03 \x03(\t\"\x93\x01\n\x1d\x41IPlatformPredictionModelSpec\x12\x12\n\nproject_id\x18\x01 \x01(\t\x12\x12\n\nmodel_name\x18\x02 \x01(\t\x12\x14\n\x0cversion_name\x18\x03 \x01(\t\x12\"\n\x18use_serialization_config\x18\x04 \x01(\x08H\x00\x42\x10\n\x0e\x65xample_config\"\x91\x01\n\x1bVertexAIPredictionModelSpec\x12\x12\n\nproject_id\x18\x01 \x01(\t\x12\x12\n\nmodel_name\x18\x02 \x01(\t\x12\x14\n\x0cversion_name\x18\x03 \x01(\t\x12\"\n\x18use_serialization_config\x18\x04 \x01(\x08H\x00\x42\x10\n\x0e\x65xample_config\"\x96\x01\n\x0f\x42\x61tchParameters\x12\x16\n\x0emin_batch_size\x18\x01 \x01(\x03\x12\x16\n\x0emax_batch_size\x18\x02 \x01(\x03\x12\x1d\n\x15target_batch_overhead\x18\x03 \x01(\x01\x12\"\n\x1atarget_batch_duration_secs\x18\x04 \x01(\x01\x12\x10\n\x08variance\x18\x05 \x01(\x01\x62\x06proto3')



_INFERENCESPECTYPE = DESCRIPTOR.message_types_by_name['InferenceSpecType']
_SAVEDMODELSPEC = DESCRIPTOR.message_types_by_name['SavedModelSpec']
_AIPLATFORMPREDICTIONMODELSPEC = DESCRIPTOR.message_types_by_name['AIPlatformPredictionModelSpec']
_VERTEXAIPREDICTIONMODELSPEC = DESCRIPTOR.message_types_by_name['VertexAIPredictionModelSpec']
_BATCHPARAMETERS = DESCRIPTOR.message_types_by_name['BatchParameters']
InferenceSpecType = _reflection.GeneratedProtocolMessageType('InferenceSpecType', (_message.Message,), {
  'DESCRIPTOR' : _INFERENCESPECTYPE,
  '__module__' : 'pipeline.components.vertex_ai_bulk_inferrer.public.proto.model_spec_pb2'
  # @@protoc_insertion_point(class_scope:pipeline.model_spec.InferenceSpecType)
  })
_sym_db.RegisterMessage(InferenceSpecType)

SavedModelSpec = _reflection.GeneratedProtocolMessageType('SavedModelSpec', (_message.Message,), {
  'DESCRIPTOR' : _SAVEDMODELSPEC,
  '__module__' : 'pipeline.components.vertex_ai_bulk_inferrer.public.proto.model_spec_pb2'
  # @@protoc_insertion_point(class_scope:pipeline.model_spec.SavedModelSpec)
  })
_sym_db.RegisterMessage(SavedModelSpec)

AIPlatformPredictionModelSpec = _reflection.GeneratedProtocolMessageType('AIPlatformPredictionModelSpec', (_message.Message,), {
  'DESCRIPTOR' : _AIPLATFORMPREDICTIONMODELSPEC,
  '__module__' : 'pipeline.components.vertex_ai_bulk_inferrer.public.proto.model_spec_pb2'
  # @@protoc_insertion_point(class_scope:pipeline.model_spec.AIPlatformPredictionModelSpec)
  })
_sym_db.RegisterMessage(AIPlatformPredictionModelSpec)

VertexAIPredictionModelSpec = _reflection.GeneratedProtocolMessageType('VertexAIPredictionModelSpec', (_message.Message,), {
  'DESCRIPTOR' : _VERTEXAIPREDICTIONMODELSPEC,
  '__module__' : 'pipeline.components.vertex_ai_bulk_inferrer.public.proto.model_spec_pb2'
  # @@protoc_insertion_point(class_scope:pipeline.model_spec.VertexAIPredictionModelSpec)
  })
_sym_db.RegisterMessage(VertexAIPredictionModelSpec)

BatchParameters = _reflection.GeneratedProtocolMessageType('BatchParameters', (_message.Message,), {
  'DESCRIPTOR' : _BATCHPARAMETERS,
  '__module__' : 'pipeline.components.vertex_ai_bulk_inferrer.public.proto.model_spec_pb2'
  # @@protoc_insertion_point(class_scope:pipeline.model_spec.BatchParameters)
  })
_sym_db.RegisterMessage(BatchParameters)

if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  _INFERENCESPECTYPE._serialized_start=99
  _INFERENCESPECTYPE._serialized_end=445
  _SAVEDMODELSPEC._serialized_start=447
  _SAVEDMODELSPEC._serialized_end=520
  _AIPLATFORMPREDICTIONMODELSPEC._serialized_start=523
  _AIPLATFORMPREDICTIONMODELSPEC._serialized_end=670
  _VERTEXAIPREDICTIONMODELSPEC._serialized_start=673
  _VERTEXAIPREDICTIONMODELSPEC._serialized_end=818
  _BATCHPARAMETERS._serialized_start=821
  _BATCHPARAMETERS._serialized_end=971
# @@protoc_insertion_point(module_scope)
